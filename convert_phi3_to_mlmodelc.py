# convert_phi3_to_mlmodelc.py

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
import coremltools as ct

# --- –ù–∞—Å—Ç—Ä–æ–π–∫–∏ ---
MODEL_NAME = "microsoft/Phi-3-mini-4k-instruct"
OUTPUT_MODEL_NAME = "Phi3Mini.mlmodelc"

print("üîÑ –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∏ –º–æ–¥–µ–ª—å...")
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)

# üî• –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: device_map=None ‚Äî –æ—Ç–∫–ª—é—á–∞–µ—Ç accelerate
model = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME,
    torch_dtype="auto",
    trust_remote_code=True,
    device_map=None,  # ‚Üê –í–ê–ñ–ù–û: –æ—Ç–∫–ª—é—á–∞–µ—Ç –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ
    low_cpu_mem_usage=True,  # –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –¥–ª—è M1
)

# –°–æ–∑–¥–∞–µ–º –ø—Ä–∏–º–µ—Ä –≤—Ö–æ–¥–∞
prompt = "Hello, how are you?"
inputs = tokenizer(prompt, return_tensors="pt")
input_ids = inputs["input_ids"]

print(f"‚úÖ –í—Ö–æ–¥–Ω–æ–π —Ç–µ–Ω–∑–æ—Ä: {input_ids.shape}")

# –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –º–æ–¥–µ–ª—å –≤ Core ML
print("üîÑ –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –º–æ–¥–µ–ª—å –≤ Core ML (—ç—Ç–æ –∑–∞–π–º–µ—Ç 5‚Äì10 –º–∏–Ω—É—Ç)...")
mlmodel = ct.convert(
    model,
    inputs=[
        ct.TensorType(
            name="input_ids",
            shape=input_ids.shape,
            dtype=input_ids.dtype
        )
    ],
    convert_to="mlprogram",
    compute_units=ct.ComputeUnit.ALL,
    skip_model_load=True
)

# –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å
print(f"üíæ –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å –∫–∞–∫ {OUTPUT_MODEL_NAME}...")
mlmodel.save(OUTPUT_MODEL_NAME)

print(f"üéâ –ì–æ—Ç–æ–≤–æ! –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {OUTPUT_MODEL_NAME}")
