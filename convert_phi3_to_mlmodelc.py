import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig
import coremltools as ct
import numpy as np

MODEL_NAME = "microsoft/Phi-3-mini-4k-instruct"
OUTPUT_MODEL_NAME = "Phi3Mini.mlpackage"  # ‚úÖ –í–ê–ñ–ù–û: .mlpackage –¥–ª—è mlprogram

# 1. –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)

# 2. –û—Ç–∫–ª—é—á–∞–µ–º rope_scaling (–æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ!)
config = AutoConfig.from_pretrained(MODEL_NAME, trust_remote_code=True)
if hasattr(config, 'rope_scaling'):
    config.rope_scaling = None

# 3. –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –≤ fp32
model = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME,
    config=config,
    torch_dtype=torch.float32,
    trust_remote_code=True,
    device_map=None,
    low_cpu_mem_usage=True,
)

model.eval()
model = model.to("cpu")

# 4. –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –ø—Ä–∏–º–µ—Ä –≤—Ö–æ–¥–∞
prompt = "Hello, how are you?"
inputs = tokenizer(prompt, return_tensors="pt", padding="max_length", max_length=128, truncation=True)
input_ids = inputs["input_ids"]      # [1, 128]
attention_mask = inputs["attention_mask"]  # [1, 128]

# 5. –û–±–µ—Ä—Ç–∫–∞ –¥–ª—è —Ç—Ä–∞—Å—Å–∏—Ä–æ–≤–∫–∏
class Phi3Wrapper(torch.nn.Module):
    def __init__(self, model):
        super().__init__()
        self.model = model

    def forward(self, input_ids, attention_mask):
        return self.model(input_ids=input_ids, attention_mask=attention_mask).logits

wrapper = Phi3Wrapper(model)

# 6. –¢—Ä–∞—Å—Å–∏—Ä–æ–≤–∫–∞ ‚Üí TorchScript
traced_model = torch.jit.trace(
    wrapper,
    (input_ids, attention_mask),
    check_trace=False,
    strict=False
)

print("‚úÖ TorchScript —Å–æ–∑–¥–∞–Ω —É—Å–ø–µ—à–Ω–æ!")

# 7. –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ Core ML (mlprogram)
mlmodel = ct.convert(
    traced_model,
    inputs=[
        ct.TensorType(name="input_ids", shape=input_ids.shape, dtype=np.int32),
        ct.TensorType(name="attention_mask", shape=attention_mask.shape, dtype=np.int32),
    ],
    convert_to="mlprogram",  # ‚úÖ –ò—Å–ø–æ–ª—å–∑—É–µ–º mlprogram
    compute_units=ct.ComputeUnit.ALL,
    skip_model_load=True,
)

# 8. –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ–º
mlmodel.save(OUTPUT_MODEL_NAME)  # ‚úÖ .mlpackage ‚Äî –û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–û!
print(f"üéâ –£—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {OUTPUT_MODEL_NAME}")
