# convert_phi3_to_mlmodelc.py

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig
import coremltools as ct

MODEL_NAME = "microsoft/Phi-3-mini-4k-instruct"
OUTPUT_MODEL_NAME = "Phi3Mini.mlmodelc"

print("üîÑ –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∏ –∫–æ–Ω—Ñ–∏–≥...")

tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)

# üîß –ö–õ–Æ–ß–ï–í–û–ô –§–ò–ö–°: –£–ë–ò–†–ê–ï–ú rope_scaling ‚Äî –æ–Ω –Ω–µ –Ω—É–∂–µ–Ω –∏ –≤—ã–∑—ã–≤–∞–µ—Ç –æ—à–∏–±–∫—É!
config = AutoConfig.from_pretrained(MODEL_NAME, trust_remote_code=True)

# –£–±–∏—Ä–∞–µ–º rope_scaling –ø–æ–ª–Ω–æ—Å—Ç—å—é ‚Äî —ç—Ç–æ –í–ê–ñ–ù–û!
if hasattr(config, 'rope_scaling'):
    config.rope_scaling = None  # ‚Üê –§–ò–ö–°: –£–ë–ò–†–ê–ï–ú –í–°–Å, —á—Ç–æ –≤—ã–∑—ã–≤–∞–µ—Ç –æ—à–∏–±–∫—É

print("‚úÖ –ö–æ–Ω—Ñ–∏–≥ –æ–±–Ω–æ–≤–ª—ë–Ω: rope_scaling = None")

# –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å —Å –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–º –∫–æ–Ω—Ñ–∏–≥–æ–º
model = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME,
    config=config,
    torch_dtype="auto",
    trust_remote_code=True,
    device_map=None,
    low_cpu_mem_usage=True,
)

# –ü—Ä–∏–º–µ—Ä –≤—Ö–æ–¥–∞
prompt = "Hello, how are you?"
inputs = tokenizer(prompt, return_tensors="pt")
input_ids = inputs["input_ids"]

print(f"‚úÖ –í—Ö–æ–¥–Ω–æ–π —Ç–µ–Ω–∑–æ—Ä: {input_ids.shape}")

# –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ Core ML
print("üîÑ –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –º–æ–¥–µ–ª—å –≤ Core ML (—ç—Ç–æ –∑–∞–π–º–µ—Ç 5‚Äì10 –º–∏–Ω—É—Ç)...")
mlmodel = ct.convert(
    model,
    inputs=[
        ct.TensorType(
            name="input_ids",
            shape=input_ids.shape,
            dtype=input_ids.dtype
        )
    ],
    convert_to="mlprogram",
    compute_units=ct.ComputeUnit.ALL,
    skip_model_load=True
)

# –°–æ—Ö—Ä–∞–Ω—è–µ–º
print(f"üíæ –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å –∫–∞–∫ {OUTPUT_MODEL_NAME}...")
mlmodel.save(OUTPUT_MODEL_NAME)

print(f"üéâ –ì–æ—Ç–æ–≤–æ! –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {OUTPUT_MODEL_NAME}")
